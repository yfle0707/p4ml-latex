\section{Introduction}
Recent success of the in-network computation has attracted more 
applications to be arised.  
\TODO{talk about the machine learning system.}
\TODO{Talk about traffic pattern: multiple workers and one parameter server.}

\switchml introduces in-network aggregation for machine learning systems
to speed up the training. The benefits from in-network computation 
come from two sides: (1) reduce network traffic; (2) reduce the computation
at the parameter servers. 
It employs a programmable switch as the parameter server instead of a host. 
\switchml targets on the rack scale. We also argue that using programmable switch
as the parameter server limits \switchml to be scaled up to 
thousands of nodes. A system which can be deployed to thousands of nodes requires:


(1) a congestion control protocol that can support the system to
co-locate with other applications, which does not use the in-network computation feature;
Isolating their traffic to the normal traffic is the state-of-the-art methodology to 
address this issue in the recent proposed in-network computaiton system~\cite{netcache, netchain, harmonia, switchml}, which wastes the network resources. 
A distributed system running in a large cluster requires the underlying networking stack equipped with 
congestion control mechanism such that they can share the network
bandwidth fairly or according to pre-configured policy.
In addition, it also requires the packet loss recovery mechanism to ensure 
reliability. Congestion collapse can happen due to the lack of congestion control, 
which finally result in low network goodput, i.e., the longer job completion time. 
The broken of the end-to-end semantics impends the congestion control mechanism to implement
in this kind of the in-network computation systems. 


(2) a resource sharing scheme that different machine learning systems/jobs
are able to share and maximally utilize the network resources, i.e., in-network computation;
To share the switch computation resources, i.e., the memory, we can pre-allocate 
the resources before a job starts, i.e., 
the memory location in switch that each packet should access, and the
size of the memory that one job can be used are fixed during the running time.
However, the machine learning jobs are running in thousands of iterations, 
the network communication
follows the classic on-off traffic pattern. The resources pre-allocated to 
a job is wasted when the job is in an communication off mode.
To maximally utilize the switch computation resources, 
the system is able to dynamically adjust resource utilization in
the dataplane when another new in-network application enters into the network.
The difficulty to achieve this goal lies on that the switch program can not dynamically 
allocate the memory in the data plane after the program starts.

(3) the system is able to be deployed to the whole data centers as
the training job size grows;
\TODO{say something that the model size grows and model becomes more sophisticated
and why do we need to support scalibility.}
We can view can reduction path from all the workers to the parameter servers as a tree structure.
The workers are the leaves, the parameter server is the root and the switches along the path 
is the node in the tree structure.
To deploy such in-network systems to the switch, it requires each hop of the switch 
to be carefully configured.
It adds the management complexity while the switch 
in the network also serves as packet switching for normal traffic.


\switchml can not achieve the above mentioned features as the programmable
switches has limited computation and memory resources.
\if 0
The state-of-the-art in-network applications use timeout to detect packet loss for the implementation ease. Packets loss recovery is a core component in
traditional TCP networking stack.
In TCP, each packet is marked by a monotonically increasing sequence number. 
If the receiver receives out-of-order packets, it sends an ACK packet with the lowest 
expected packet sequence number to the sender. 
When the sender receives three duplicated ACKs, it assumes a packet loss happens.
While a packet may be consumed at switch, it can cause the confusion at the receiver
that a packet loss happened. It is challenging to design loss recovery due to that the end-to-end semantics are broken 
for these new applications.
 
Finally, these in-network applications assume that the switch always has the resources to do the aggregation,
which is not true as more in-network applications enters into the network. Thus, it requires the applications or
the underlying protocol to be able to handle the best efforts case, where the network may or may not do the 
aggregation for the application. This relaxed the restriction that the in-network applications has to be deployed 
in a programmable switch or a switch has the in-network aggregation program loaded.   
\fi 
This motivates us to design a new architecture, called \system, to meet the following requirements such that it can (1) be capable with normal applications; (2) support dynamic resource sharing with in-network applications in the data plane;
(3) support large scale deployment; (4) be easily extended to a general framework that can used in other applications~\TODO{need more thinking.}. 

\system obeys the end-to-end principle and uses the end hosts as workers and parameter servers. 
The end-to-end principle enables us to implement the congestion control and packet loss recovery 
naturally at the end host. A straightforward approach could totally employ 
the congestion control and packet loss recovery from traditional TCP stack.
In TCP, each packet is marked by a monotonically increasing sequence number. 
If the receiver receives out-of-order packets, it sends an ACK packet with the lowest 
expected packet sequence number to the sender. 
When the sender receives three duplicated ACKs, it assumes a packet loss happens.
While a packet may be consumed at switch, it can cause the confusion at the receiver
that a packet loss happened. It is challenging to design loss recovery due to that the end-to-end semantics are broken even we still use the hosts as the end points as the traditional machine learning systems. 
 
Secondly, as the in-network aggregation requires maintaining the state in the switch, 
we need \TODO{need to be specific.}

Thirdly, the management complexity on the switch to deploy 
the in-network aggregation system on the whole data centers 
are still requires to be addressed.
\TODO{need more specific.}      
